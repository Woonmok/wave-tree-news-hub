#!/usr/bin/env python3
# news_hub.py (ë¡œì»¬ ê·œì¹™ ê¸°ë°˜ ë‰´ìŠ¤ ë¶„ì„ + Daily Bridge)
import os
import re
from dotenv import load_dotenv
from datetime import datetime
import json
import shutil
import requests

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ê²½ë¡œ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
WORK_DIR = BASE_DIR
DAILY_BRIDGE_PATH = os.path.join(BASE_DIR, "Daily_Bridge.md")

ANTIGRAVITY_PATH = os.getenv("ANTIGRAVITY_PATH", "").strip()
if not ANTIGRAVITY_PATH:
    workspace_root = os.path.abspath(os.path.join(BASE_DIR, ".."))
    ANTIGRAVITY_PATH = os.path.join(workspace_root, "woonmok.github.io", "Project_Radar.md")

# ===== ì„¤ì • =====
KEYWORDS = [
    "ê· ì‚¬ì²´", "mycelium", "ë°°ì–‘ìœ¡", "cultured meat",
    "ì§„ì•ˆ", "POM", "Bio-R&D", "ë°”ì´ì˜¤",
    "cell-based", "fermentation", "ë°°ì–‘",
    "listeria", "ë¦¬ìŠ¤í…Œë¦¬ì•„", "ê³ ê¸‰ ì˜¤ë””ì˜¤", "í•˜ì´ì—”ë“œ",
    "ai", "computer", "gpu", "blackwell"
]

EXCLUDE_KEYWORDS = [
    "ê´‘ê³ ", "ìŠ¤í°ì„œ", "sponsored", "promo", "affiliate"
]


def score_news(news_text, matched_keywords):
    """ë¡œì»¬ ì ìˆ˜ ê³„ì‚°"""
    text_lower = news_text.lower()
    score = 5.0

    high_impact = ["ì ˆê°", "íš¨ìœ¨", "í˜ì‹ ", "ê¸´ê¸‰", "fda", "blackwell", "ë°°ì–‘ìœ¡", "ê· ì‚¬ì²´"]
    for keyword in high_impact:
        if keyword.lower() in text_lower:
            score += 0.8

    score += min(len(matched_keywords) * 0.6, 2.0)
    score = max(1.0, min(10.0, score))
    return round(score, 1)

# 1. í‚¤ì›Œë“œ í•„í„°ë§ í•¨ìˆ˜
def filter_by_keywords(news_text, keywords=KEYWORDS, exclude=EXCLUDE_KEYWORDS):
    """íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë‰´ìŠ¤ë§Œ ì„ íƒ"""
    text_lower = news_text.lower()
    
    # ì œì™¸ í‚¤ì›Œë“œ í™•ì¸
    for exclude_kw in exclude:
        if exclude_kw.lower() in text_lower:
            return False, "ì œì™¸ í‚¤ì›Œë“œ í¬í•¨"
    
    # í¬í•¨ í‚¤ì›Œë“œ í™•ì¸
    matched_keywords = [kw for kw in keywords if kw.lower() in text_lower]
    if matched_keywords:
        return True, matched_keywords
    
    return False, "ê´€ë ¨ í‚¤ì›Œë“œ ì—†ìŒ"


# 2. ì •ë³´ ìˆ˜ì§‘ (RSS/API)
def fetch_news():
    """ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ (2026ë…„ ì‹œì¥ íŠ¸ë Œë“œ ì‹œë®¬ë ˆì´ì…˜)"""
    # ì‹¤ì œ ìš´ì˜ ì‹œ: NewsAPIë‚˜ RSS í”¼ë“œë¥¼ ì—°ë™í•©ë‹ˆë‹¤.
    sample_news = [
        "ë¯¸êµ­ ë‚´ ë°°ì–‘ìœ¡ ì‹œì¥, ê³ ë¹„ìš© ë¬¸ì œë¡œ ì„¸í¬ ë°°ì–‘ ë°©ì‹ì—ì„œ ê· ì‚¬ì²´(Mycelium) ê¸°ë°˜ ë°œíš¨ ë°©ì‹ìœ¼ë¡œ ê¸‰ê²©í•œ ì´ë™ ì¤‘",
        "Better Meat Co ë° Prime Roots, ì‚°ì—…ìš© ì—°ì† ë°œíš¨ ì‹œìŠ¤í…œ ë„ì…ìœ¼ë¡œ ìƒì‚° ë‹¨ê°€ 30% ì ˆê° ì„±ê³µ",
        "2026ë…„ í‘¸ë“œí…Œí¬ íŠ¸ë Œë“œ: 'Precision Fermentation'ê³¼ ë²„ì„¯ ê· ì‚¬ì²´ë¥¼ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ë‹¨ë°±ì§ˆ ë¶€ìƒ",
        "FDA ë¦¬ìŠ¤í…Œë¦¬ì•„ ê¸´ê¸‰ ì•Œë¦¼ ë°œí‘œ - ëƒ‰ì¥ ì‹í’ˆ ê´€ë ¨",
        "ê³ ê¸‰ ì˜¤ë””ì˜¤ ê¸°ìˆ  ìµœì‹  ë™í–¥ - DSD í¬ë§· ì£¼ë¥˜í™”",
        "NVIDIA Blackwell GPU, AI ì¸í”„ë¼ í˜ì‹  ì£¼ë„",
        "ìŠ¤íƒ€íŠ¸ì—… ê´‘ê³ : ìƒˆ ì œí’ˆ ì¶œì‹œ ìŠ¤í°ì„œë¨ (ì œì™¸ ëŒ€ìƒ)",
    ]
    return sample_news


# 3. ì „ëµì  í•„í„°ë§ (ë¡œì»¬ ê·œì¹™ ê¸°ë°˜)
def analyze_importance(news_text, matched_keywords):
    """ë¡œì»¬ ê·œì¹™ ê¸°ë°˜ ë‰´ìŠ¤ ì¤‘ìš”ë„ ë¶„ì„"""
    score = score_news(news_text, matched_keywords)
    title = news_text[:48] + ("..." if len(news_text) > 48 else "")

    actions = []
    lower_text = news_text.lower()
    if any(keyword in lower_text for keyword in ["ì ˆê°", "íš¨ìœ¨", "ë°œíš¨"]):
        actions.append("ë¹„ìš© ì ˆê° PoC ìš°ì„  ê²€í† ")
    if any(keyword in lower_text for keyword in ["ë¦¬ìŠ¤í…Œë¦¬ì•„", "listeria", "fda", "ê¸´ê¸‰"]):
        actions.append("ì‹í’ˆì•ˆì „ ëª¨ë‹ˆí„°ë§ ì¹´ë“œ ì¦‰ì‹œ ìƒì„±")
    if any(keyword in lower_text for keyword in ["gpu", "blackwell", "ai", "ì¸í”„ë¼"]):
        actions.append("ì¸í”„ë¼ íˆ¬ì/ì—…ê·¸ë ˆì´ë“œ ì˜í–¥ë„ ê³„ì‚°")
    if not actions:
        actions.append("ì£¼ê°„ íšŒì˜ ì•ˆê±´ìœ¼ë¡œ ì¶”ì ")

    return (
        f"[{score}/10] | {title}\n"
        f"ë¶„ì„: ê°ì§€ í‚¤ì›Œë“œ({', '.join(matched_keywords)}) ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ê´€ë ¨ì„±ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
        f"ì•¡ì…˜: {actions[0]}"
    )


# 3-1. Daily Bridge ìƒì„± í•¨ìˆ˜ (ìƒˆë¡œìš´ ê¸°ëŠ¥!)
def create_daily_bridge(news_data_list):
    """
    ë§¤ì¼ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì¤‘ TOP 3ì„ ì •ì œí•˜ì—¬ Daily_Bridge.md ìƒì„±
    ì´ íŒŒì¼ì´ VS Code â†” Antigravity ì—°ê²°ì 
    """
    if not news_data_list:
        print("   âš ï¸ ë¶„ì„í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    timestamp = datetime.now().strftime("%Yë…„ %mì›” %dì¼ %H:%M:%S")
    
    ranked = sorted(
        news_data_list,
        key=lambda item: score_news(item.get("text", ""), item.get("keywords", [])),
        reverse=True,
    )[:3]

    sections = ["## ë ˆì´ë” ê°ì§€ ê²°ê³¼ (TOP 3)", ""]
    for index, item in enumerate(ranked, 1):
        text = item.get("text", "")
        keywords = item.get("keywords", [])
        score = score_news(text, keywords)
        action_line = "ì£¼ê°„ íŠ¸ë˜í‚¹ ìœ ì§€"
        lower_text = text.lower()
        if any(keyword in lower_text for keyword in ["ì ˆê°", "íš¨ìœ¨", "ë°œíš¨"]):
            action_line = "ë¹„ìš© ì ˆê° ì‹¤í—˜ í•­ëª© ìš°ì„  ë°°ì¹˜"
        elif any(keyword in lower_text for keyword in ["ë¦¬ìŠ¤í…Œë¦¬ì•„", "listeria", "fda", "ê¸´ê¸‰"]):
            action_line = "ì‹í’ˆì•ˆì „ ëŒ€ì‘ ì‹œë‚˜ë¦¬ì˜¤ ì ê²€"
        elif any(keyword in lower_text for keyword in ["gpu", "blackwell", "ai", "ì¸í”„ë¼"]):
            action_line = "ì„œë²„ ì¸í”„ë¼ ëŒ€ì‘ ê³„íš ì—…ë°ì´íŠ¸"

        title = text[:45] + ("..." if len(text) > 45 else "")
        sections.extend([
            f"### {index}ï¸âƒ£ {title}",
            f"- ì›ë¬¸: {text}",
            f"- ì˜í–¥ë„: {score}/10",
            f"- ì‹¤í–‰ ì¸ì‚¬ì´íŠ¸: {action_line}",
            "",
        ])

    bridge_content = "\n".join(sections).strip()
    
    # Daily_Bridge.md ìƒì„±
    full_content = f"""# ğŸ“¡ Daily Bridge - {timestamp}

**ì´ íŒŒì¼ì€ VS Codeì™€ Antigravityë¥¼ ì—°ê²°í•˜ëŠ” ì¸ì‚¬ì´íŠ¸ ë¸Œë¦¿ì§€ì…ë‹ˆë‹¤.**

{bridge_content}

---

## ë‹¤ìŒ ë‹¨ê³„
ì´ ë‚´ìš©ì„ Antigravityì— ë³µì‚¬í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ ì§ˆë¬¸í•˜ì„¸ìš”:
> "ì˜¤ëŠ˜ì˜ ë ˆì´ë” ê°ì§€ ê²°ê³¼ì•¼. 
> ì´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í˜„ì¬ Wave Tree Project Dashboardì—ì„œ 
> ìˆ˜ì •í•˜ê±°ë‚˜ ìƒˆë¡œ ì¶”ê°€í•´ì•¼ í•  To-Do ì¹´ë“œ 3ê°œë¥¼ ë½‘ì•„ì¤˜."

ìƒì„± ì‹œê°: {timestamp}
"""
    
    # íŒŒì¼ ì €ì¥
    try:
        with open(DAILY_BRIDGE_PATH, "w", encoding="utf-8") as f:
            f.write(full_content)
        print(f"   âœ… Daily_Bridge.md ìƒì„± ì™„ë£Œ: {DAILY_BRIDGE_PATH}")
        return DAILY_BRIDGE_PATH
    except Exception as e:
        print(f"   âš ï¸ Daily_Bridge.md ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        return None


def append_daily_bridge_to_news_json(bridge_path, category="global_biz"):
    if not bridge_path or not os.path.exists(bridge_path):
        print("   âš ï¸ Daily Bridge íŒŒì¼ì´ ì—†ì–´ news.json ì¶”ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return False

    news_json_path = os.path.join(BASE_DIR, "data", "normalized", "news.json")

    try:
        with open(bridge_path, "r", encoding="utf-8") as f:
            content = f.read()
    except Exception as e:
        print(f"   âš ï¸ Daily Bridge ì½ê¸° ì‹¤íŒ¨: {str(e)}")
        return False

    date_match = re.search(r"(\d{4})ë…„\s*(\d{2})ì›”\s*(\d{2})ì¼", content)
    if date_match:
        date_str = f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}"
    else:
        date_str = datetime.now().strftime("%Y-%m-%d")

    bridge_id = f"daily_bridge_{date_str}"
    title = f"Daily Bridge {date_str}"

    bullets = []
    for line in content.splitlines():
        text = line.strip()
        if text.startswith("*"):
            bullets.append(text.lstrip("* ").strip())
        if len(bullets) >= 3:
            break

    summary = " ".join(bullets).strip()
    if not summary:
        summary = content.replace("\n", " ")
    summary = " ".join(summary.split()).strip()
    summary = summary[:180]

    try:
        if os.path.exists(news_json_path):
            with open(news_json_path, "r", encoding="utf-8") as f:
                data = json.load(f)
        else:
            data = {"generated_at": datetime.now().isoformat(), "items": []}

        items = data.get("items", [])
        if any(str(item.get("id")) == bridge_id for item in items):
            print("   â„¹ï¸ Daily Bridgeê°€ ì´ë¯¸ news.jsonì— ì¡´ì¬í•©ë‹ˆë‹¤.")
            return False

        items.insert(0, {
            "id": bridge_id,
            "category": category,
            "title": title,
            "source": "Daily_Bridge",
            "url": None,
            "published_at": datetime.now().isoformat(),
            "summary": summary,
            "highlights": [],
            "tags": ["daily_bridge"],
            "score": 0.95
        })

        data["generated_at"] = datetime.now().isoformat()
        data["items"] = items

        os.makedirs(os.path.dirname(news_json_path), exist_ok=True)
        with open(news_json_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"   âœ… Daily Bridgeê°€ news.jsonì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤: {news_json_path}")
        return True
    except Exception as e:
        print(f"   âš ï¸ news.json ì¶”ê°€ ì‹¤íŒ¨: {str(e)}")
        return False


# 4. ê²°ê³¼ ì €ì¥ (Markdown)
def save_to_radar(news_text, matched_keywords, analysis=None):
    """Project_Radar.mdì— ê²°ê³¼ ì €ì¥ ë° Antigravityë¡œ ìë™ ë™ê¸°í™”"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    radar_file = "Project_Radar.md"
    
    # íŒŒì¼ì´ ì—†ìœ¼ë©´ í—¤ë” ìƒì„±
    if not os.path.exists(radar_file):
        with open(radar_file, "w", encoding="utf-8") as f:
            f.write("# Project Radar - ë‰´ìŠ¤ ê°ì§€ ë¡œê·¸\n\n")
    
    with open(radar_file, "a", encoding="utf-8") as f:
        f.write(f"## [{timestamp}] ì‹ ê·œ ê°ì§€\n")
        f.write(f"**ë‰´ìŠ¤**: {news_text[:100]}...\n\n")
        f.write(f"**ê°ì§€ í‚¤ì›Œë“œ**: {', '.join(matched_keywords)}\n\n")
        if analysis:
            f.write(f"**ë¶„ì„**: {analysis}\n")
        f.write("\n---\n\n")
    
    # Antigravityë¡œ ìë™ ë™ê¸°í™”
    try:
        shutil.copy2(radar_file, ANTIGRAVITY_PATH)
        print(f"   ğŸ”„ Antigravity ë™ê¸°í™” ì™„ë£Œ: {ANTIGRAVITY_PATH}")
    except Exception as e:
        print(f"   âš ï¸ Antigravity ë™ê¸°í™” ì‹¤íŒ¨: {str(e)}")


# 5. JSONìœ¼ë¡œë„ ì €ì¥ (API ì—°ë™ ìš©)
def save_to_json(news_data):
    """ê°ì§€ëœ ë‰´ìŠ¤ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
    json_file = "detected_news.json"
    
    try:
        with open(json_file, "r", encoding="utf-8") as f:
            data = json.load(f)
    except FileNotFoundError:
        data = []
    
    data.append({
        "timestamp": datetime.now().isoformat(),
        "news": news_data["text"],
        "keywords": news_data["keywords"],
        "analysis": news_data.get("analysis", "")
    })
    
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


# Dashboard ì—…ë°ì´íŠ¸ í•¨ìˆ˜
def update_dashboard(news_data_list):
    """
    dashboard_data.jsonì˜ intelligence ì„¹ì…˜ì„ ìµœì‹  ë‰´ìŠ¤ë¡œ ì—…ë°ì´íŠ¸
    """
    dashboard_env = os.getenv("DASHBOARD_DATA_PATH", "").strip()
    if dashboard_env:
        DASHBOARD_PATH = dashboard_env
    else:
        workspace_root = os.path.abspath(os.path.join(BASE_DIR, ".."))
        DASHBOARD_PATH = os.path.join(workspace_root, "woonmok.github.io", "dashboard_data.json")
    
    if not news_data_list:
        print("   âš ï¸ ì—…ë°ì´íŠ¸í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    try:
        # ê¸°ì¡´ dashboard_data.json ë¡œë“œ
        if os.path.exists(DASHBOARD_PATH):
            with open(DASHBOARD_PATH, 'r', encoding='utf-8') as f:
                dashboard_data = json.load(f)
        else:
            dashboard_data = {
                "todo_list": [],
                "system_status": "NORMAL",
                "intelligence": []
            }
        
        # ìƒìœ„ 3ê°œ ë‰´ìŠ¤ ì¶”ì¶œ
        top_news = []
        for item in news_data_list[:3]:
            title = item.get('text', '')[:100]  # ì œëª© ì¶”ì¶œ
            analysis = item.get('analysis', '')
            
            # ê°„ë‹¨í•œ ìš”ì•½ ì¶”ì¶œ (ì²« ë¬¸ì¥)
            summary = analysis.split('.')[0] if analysis else "ë¶„ì„ ì¤‘"
            
            # ì¹´í…Œê³ ë¦¬ íŒë‹¨
            keywords = item.get('keywords', [])
            if any(kw in ['listeria', 'ë¦¬ìŠ¤í…Œë¦¬ì•„'] for kw in keywords):
                tag = "ê¸´ê¸‰"
                score = "0.95"
            elif any(kw in ['ë°°ì–‘ìœ¡', 'cultured meat', 'ê· ì‚¬ì²´'] for kw in keywords):
                tag = "ì¤‘ìš”"
                score = "0.85"
            else:
                tag = "ì •ë³´"
                score = "0.75"
            
            top_news.append({
                "title": title,
                "summary": summary[:150],
                "tag": tag,
                "score": score
            })
        
        # intelligence ì„¹ì…˜ ì—…ë°ì´íŠ¸
        dashboard_data["intelligence"] = top_news
        dashboard_data["last_updated"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # ì €ì¥
        with open(DASHBOARD_PATH, 'w', encoding='utf-8') as f:
            json.dump(dashboard_data, f, ensure_ascii=False, indent=2)
        
        print(f"   âœ… Dashboard ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(top_news)}ê°œ ë‰´ìŠ¤")
        
    except Exception as e:
        print(f"   âš ï¸ Dashboard ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {str(e)}")


# 6. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def process_news(use_local_analysis=True):
    """ë‰´ìŠ¤ í•„í„°ë§ ë° ë¶„ì„ ë©”ì¸ í•¨ìˆ˜ + Daily_Bridge.md ìƒì„±"""
    news_list = fetch_news()
    processed_count = 0
    skipped_count = 0
    processed_news_data = []
    
    print("=" * 60)
    print("ğŸ›°ï¸ ì™¸ë¶€ ì •ë³´ ê°ì§€ ì‹œìŠ¤í…œ ê°€ë™ ì¤‘...")
    print("=" * 60)
    print(f"ğŸ“‹ ê°ì§€ í‚¤ì›Œë“œ ({len(KEYWORDS)}ê°œ): {', '.join(KEYWORDS[:5])}...")
    print(f"ğŸš« ì œì™¸ í‚¤ì›Œë“œ ({len(EXCLUDE_KEYWORDS)}ê°œ): {', '.join(EXCLUDE_KEYWORDS)}\n")
    
    for idx, news in enumerate(news_list, 1):
        print(f"\n[{idx}/{len(news_list)}] ì²˜ë¦¬ ì¤‘...")
        print(f"   ğŸ“ ë‰´ìŠ¤: {news[:60]}...")
        
        # í‚¤ì›Œë“œ í•„í„°ë§
        is_relevant, result = filter_by_keywords(news)
        
        if not is_relevant:
            print(f"   âœ— ê±´ë„ˆëœ€: {result}")
            skipped_count += 1
            continue
        
        matched_keywords = result
        print(f"   âœ“ í•„í„° í†µê³¼!")
        print(f"   ğŸ¯ ê°ì§€ëœ í‚¤ì›Œë“œ: {', '.join(matched_keywords)}")
        
        # ë¡œì»¬ ë¶„ì„
        analysis = None
        try:
            print(f"   ğŸ”„ ë¡œì»¬ ë¶„ì„ ì§„í–‰ ì¤‘...")
            analysis = analyze_importance(news, matched_keywords)
            print(f"   âœ… ë¶„ì„ ì™„ë£Œ")
        except Exception as e:
            print(f"   âš ï¸ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        
        # Markdown ì €ì¥
        try:
            save_to_radar(news, matched_keywords, analysis)
            print(f"   ğŸ’¾ Markdown ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            print(f"   âš ï¸ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
        
        # JSON ì €ì¥
        try:
            save_to_json({
                "text": news,
                "keywords": matched_keywords,
                "analysis": analysis or ""
            })
            print(f"   ğŸ’¾ JSON ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            print(f"   âš ï¸ JSON ì €ì¥ ì˜¤ë¥˜: {str(e)}")
        
        # Daily Bridge ìƒì„±ìš© ë°ì´í„° ìˆ˜ì§‘
        processed_news_data.append({
            "text": news,
            "keywords": matched_keywords,
            "analysis": analysis
        })
        
        processed_count += 1
    
    # Daily_Bridge.md ìƒì„± (í•µì‹¬!)
    print("\n" + "=" * 60)
    print("ğŸŒ‰ Daily Bridge ìƒì„± ì¤‘...")
    print("=" * 60)
    bridge_path = create_daily_bridge(processed_news_data)

    # Daily_Bridge.md -> news.json append
    if bridge_path:
        append_daily_bridge_to_news_json(bridge_path, category="global_biz")
    
    # Dashboard ì—…ë°ì´íŠ¸
    print("\n" + "=" * 60)
    print("ğŸ“Š Dashboard ì—…ë°ì´íŠ¸ ì¤‘...")
    print("=" * 60)
    update_dashboard(processed_news_data)
    
    # Intelligence Hub ì—…ë°ì´íŠ¸ (index.html)
    print("\n" + "=" * 60)
    print("ğŸŒ Intelligence Hub ì—…ë°ì´íŠ¸ ì¤‘...")
    print("=" * 60)
    try:
        from sync_top_news import sync_to_html
        sync_to_html()
        print("   âœ… Intelligence Hub ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    except Exception as e:
        print(f"   âš ï¸ Intelligence Hub ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {str(e)}")
    
    print("\n" + "=" * 60)
    print(f"âœ… ë¶„ì„ ì™„ë£Œ. ëª¨ë“  íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"   âœ“ ì €ì¥ë¨: {processed_count}ê°œ")
    print(f"   âœ— ê±´ë„ˆëœ€: {skipped_count}ê°œ")
    print(f"   ğŸ“ ìƒì„± íŒŒì¼:")
    print(f"      - Project_Radar.md (Antigravity ë™ê¸°í™”)")
    print(f"      - detected_news.json (API ì—°ë™)")
    print(f"      - Daily_Bridge.md â­ (VS Code â†” Antigravity ë¸Œë¦¿ì§€)")
    print(f"      - dashboard_data.json â­ (ëŒ€ì‹œë³´ë“œ ë™ê¸°í™”)")
    print(f"      - index.html Intelligence Hub â­ (ì›¹ì‚¬ì´íŠ¸ ë™ê¸°í™”)")
    print("=" * 60)


# ì‹¤í–‰
if __name__ == "__main__":
    process_news(use_local_analysis=True)
